{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.dirname(os.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cybaj/projects/studies/venv_python/venv_py27_nltk/lib/python2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories: 15\n",
      "[u'adventure', u'belles_lettres', u'editorial', u'fiction', u'government', u'hobbies', u'humor', u'learned', u'lore', u'mystery', u'news', u'religion', u'reviews', u'romance', u'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Oct 04 23:08:05 2016\n",
    "\n",
    "@author: DIP\n",
    "\"\"\"\n",
    "\n",
    "# BROWN CORPUS DEMO\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "\n",
    "print 'Total Categories:', len(brown.categories())\n",
    "\n",
    "print brown.categories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on CategorizedTaggedCorpusReader in module nltk.corpus.reader.tagged object:\n",
      "\n",
      "class CategorizedTaggedCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, TaggedCorpusReader)\n",
      " |  A reader for part-of-speech tagged corpora whose documents are\n",
      " |  divided into categories based on their file identifiers.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CategorizedTaggedCorpusReader\n",
      " |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      " |      TaggedCorpusReader\n",
      " |      nltk.corpus.reader.api.CorpusReader\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initialize the corpus reader.  Categorization arguments\n",
      " |      (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      " |      the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      " |      are passed to the ``TaggedCorpusReader``.\n",
      " |  \n",
      " |  paras(self, fileids=None, categories=None)\n",
      " |  \n",
      " |  raw(self, fileids=None, categories=None)\n",
      " |  \n",
      " |  sents(self, fileids=None, categories=None)\n",
      " |  \n",
      " |  tagged_paras(self, fileids=None, categories=None, tagset=None)\n",
      " |  \n",
      " |  tagged_sents(self, fileids=None, categories=None, tagset=None)\n",
      " |  \n",
      " |  tagged_words(self, fileids=None, categories=None, tagset=None)\n",
      " |  \n",
      " |  words(self, fileids=None, categories=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      " |  \n",
      " |  categories(self, fileids=None)\n",
      " |      Return a list of the categories that are defined for this corpus,\n",
      " |      or for the file(s) if it is given.\n",
      " |  \n",
      " |  fileids(self, categories=None)\n",
      " |      Return a list of file identifiers for the files that make up\n",
      " |      this corpus, or that make up the given category(s) if specified.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      x.__str__() <==> str(x)\n",
      " |  \n",
      " |  __unicode__ = __str__(...)\n",
      " |      x.__str__() <==> str(x)\n",
      " |  \n",
      " |  abspath(self, fileid)\n",
      " |      Return the absolute path for the given file.\n",
      " |      \n",
      " |      :type fileid: str\n",
      " |      :param fileid: The file identifier for the file whose path\n",
      " |          should be returned.\n",
      " |      :rtype: PathPointer\n",
      " |  \n",
      " |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      " |      Return a list of the absolute paths for all fileids in this corpus;\n",
      " |      or for the given list of fileids, if specified.\n",
      " |      \n",
      " |      :type fileids: None or str or list\n",
      " |      :param fileids: Specifies the set of fileids for which paths should\n",
      " |          be returned.  Can be None, for all fileids; a list of\n",
      " |          file identifiers, for a specified set of fileids; or a single\n",
      " |          file identifier, for a single file.  Note that the return\n",
      " |          value is always a list of paths, even if ``fileids`` is a\n",
      " |          single file identifier.\n",
      " |      \n",
      " |      :param include_encoding: If true, then return a list of\n",
      " |          ``(path_pointer, encoding)`` tuples.\n",
      " |      \n",
      " |      :rtype: list(PathPointer)\n",
      " |  \n",
      " |  citation(self)\n",
      " |      Return the contents of the corpus citation.bib file, if it exists.\n",
      " |  \n",
      " |  encoding(self, file)\n",
      " |      Return the unicode encoding for the given corpus file, if known.\n",
      " |      If the encoding is unknown, or if the given file should be\n",
      " |      processed using byte strings (str), then return None.\n",
      " |  \n",
      " |  ensure_loaded(self)\n",
      " |      Load this corpus (if it has not already been loaded).  This is\n",
      " |      used by LazyCorpusLoader as a simple method that can be used to\n",
      " |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      " |      do help(some_corpus).\n",
      " |  \n",
      " |  license(self)\n",
      " |      Return the contents of the corpus LICENSE file, if it exists.\n",
      " |  \n",
      " |  open(self, file)\n",
      " |      Return an open stream that can be used to read the given file.\n",
      " |      If the file's encoding is not None, then the stream will\n",
      " |      automatically decode the file's contents into unicode.\n",
      " |      \n",
      " |      :param file: The file identifier of the file to read.\n",
      " |  \n",
      " |  readme(self)\n",
      " |      Return the contents of the corpus README file, if it exists.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      " |  \n",
      " |  root\n",
      " |      The directory where this corpus is stored.\n",
      " |      \n",
      " |      :type: PathPointer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(brown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'There', u'were', u'thirty-eight', u'patients', u'on', u'the', u'bus', u'the', u'morning', u'I', u'left', u'for', u'Hanover', u',', u'most', u'of', u'them', u'disturbed', u'and', u'hallucinating', u'.'], [u'An', u'interne', u',', u'a', u'nurse', u'and', u'two', u'attendants', u'were', u'in', u'charge', u'of', u'us', u'.'], ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# tokenized sentences\n",
    "brown.sents(categories='mystery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'There', u'EX'), (u'were', u'BED'), (u'thirty-eight', u'CD'), (u'patients', u'NNS'), (u'on', u'IN'), (u'the', u'AT'), (u'bus', u'NN'), (u'the', u'AT'), (u'morning', u'NN'), (u'I', u'PPSS'), (u'left', u'VBD'), (u'for', u'IN'), (u'Hanover', u'NP'), (u',', u','), (u'most', u'AP'), (u'of', u'IN'), (u'them', u'PPO'), (u'disturbed', u'VBN'), (u'and', u'CC'), (u'hallucinating', u'VBG'), (u'.', u'.')], [(u'An', u'AT'), (u'interne', u'NN'), (u',', u','), (u'a', u'AT'), (u'nurse', u'NN'), (u'and', u'CC'), (u'two', u'CD'), (u'attendants', u'NNS'), (u'were', u'BED'), (u'in', u'IN'), (u'charge', u'NN'), (u'of', u'IN'), (u'us', u'PPO'), (u'.', u'.')], ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# POS tagged sentences\n",
    "brown.tagged_sents(categories='mystery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences in natural form\n",
    "sentences = brown.sents(categories='mystery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tagged words\n",
    "tagged_words = brown.tagged_words(categories='mystery')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'patients', u'NNS'), (u'bus', u'NN'), (u'morning', u'NN'), (u'Hanover', u'NP'), (u'interne', u'NN'), (u'nurse', u'NN'), (u'attendants', u'NNS'), (u'charge', u'NN'), (u'bus', u'NN'), (u'window', u'NN')]\n"
     ]
    }
   ],
   "source": [
    "# get nouns from tagged words\n",
    "nouns = [(word, tag) for word, tag in tagged_words if any(noun_tag in tag for noun_tag in ['NP', 'NN'])]\n",
    "\n",
    "print nouns[0:10] # prints the first 10 nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'man', 106), (u'time', 82), (u'door', 80), (u'car', 69), (u'room', 65), (u'Mr.', 63), (u'way', 61), (u'office', 50), (u'eyes', 48), (u'hand', 46)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build frequency distribution for nouns\n",
    "nouns_freq = nltk.FreqDist([word for word, tag in nouns])\n",
    "\n",
    "# print top 10 occuring nouns\n",
    "print nouns_freq.most_common(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories: 90\n"
     ]
    }
   ],
   "source": [
    "# REUTERS CORPUS DEMO\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "print 'Total Categories:', len(reuters.categories())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'acq', u'alum', u'barley', u'bop', u'carcass', u'castor-oil', u'cocoa', u'coconut', u'coconut-oil', u'coffee', u'copper', u'copra-cake', u'corn', u'cotton', u'cotton-oil', u'cpi', u'cpu', u'crude', u'dfl', u'dlr', u'dmk', u'earn', u'fuel', u'gas', u'gnp', u'gold', u'grain', u'groundnut', u'groundnut-oil', u'heat', u'hog', u'housing', u'income', u'instal-debt', u'interest', u'ipi', u'iron-steel', u'jet', u'jobs', u'l-cattle', u'lead', u'lei', u'lin-oil', u'livestock', u'lumber', u'meal-feed', u'money-fx', u'money-supply', u'naphtha', u'nat-gas', u'nickel', u'nkr', u'nzdlr', u'oat', u'oilseed', u'orange', u'palladium', u'palm-oil', u'palmkernel', u'pet-chem', u'platinum', u'potato', u'propane', u'rand', u'rape-oil', u'rapeseed', u'reserves', u'retail', u'rice', u'rubber', u'rye', u'ship', u'silver', u'sorghum', u'soy-meal', u'soy-oil', u'soybean', u'strategic-metal', u'sugar', u'sun-meal', u'sun-oil', u'sunseed', u'tea', u'tin', u'trade', u'veg-oil', u'wheat', u'wpi', u'yen', u'zinc']\n"
     ]
    }
   ],
   "source": [
    "print reuters.categories()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"YUGOSLAV ECONOMY WORSENED IN 1986 , BANK DATA SHOWS National Bank economic data for 1986 shows that Yugoslavia ' s trade deficit grew , the inflation rate rose , wages were sharply higher , the money supply expanded and the value of the dinar fell .\", u'The trade deficit for 1986 was 2 . 012 billion dlrs , 25 . 7 pct higher than in 1985 .', u'The trend continued in the first three months of this year as exports dropped by 17 . 8 pct , in hard currency terms , to 2 . 124 billion dlrs .', u'Yugoslavia this year started quoting trade figures in dinars based on current exchange rates , instead of dollars based on a fixed exchange rate of 264 . 53 dinars per dollar .', u\"Yugoslavia ' s balance of payments surplus with the convertible currency area fell to 245 mln dlrs in 1986 from 344 mln in 1985 .\"]\n"
     ]
    }
   ],
   "source": [
    "# get sentences in housing and income categories\n",
    "sentences = reuters.sents(categories=['housing', 'income'])\n",
    "sentences = [' '.join(sentence_tokens) for sentence_tokens in sentences]\n",
    "\n",
    "print sentences[0:5]  # prints the first 5 sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'test/16118', u'test/18534', u'test/18540', u'test/18664', u'test/18665', u'test/18672', u'test/18911', u'test/19875', u'test/20106', u'test/20116', u'training/1035', u'training/1036', u'training/10602', u'training/10604', u'training/11170', u'training/11665', u'training/2618', u'training/29', u'training/3105', u'training/3708', u'training/3720', u'training/3723', u'training/3898', u'training/5883', u'training/5886', u'training/6000', u'training/6067', u'training/6197', u'training/7005', u'training/7006', u'training/7015', u'training/7036', u'training/7098', u'training/7099', u'training/9615']\n",
      "[[u'YUGOSLAV', u'ECONOMY', u'WORSENED', u'IN', u'1986', u',', u'BANK', u'DATA', u'SHOWS', u'National', u'Bank', u'economic', u'data', u'for', u'1986', u'shows', u'that', u'Yugoslavia', u\"'\", u's', u'trade', u'deficit', u'grew', u',', u'the', u'inflation', u'rate', u'rose', u',', u'wages', u'were', u'sharply', u'higher', u',', u'the', u'money', u'supply', u'expanded', u'and', u'the', u'value', u'of', u'the', u'dinar', u'fell', u'.'], [u'The', u'trade', u'deficit', u'for', u'1986', u'was', u'2', u'.', u'012', u'billion', u'dlrs', u',', u'25', u'.', u'7', u'pct', u'higher', u'than', u'in', u'1985', u'.'], ...]\n"
     ]
    }
   ],
   "source": [
    "# fileid based access\n",
    "print reuters.fileids(categories=['housing', 'income'])\n",
    "\n",
    "print reuters.sents(fileids=[u'test/16118', u'test/18534'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('hike.n.01'), Synset('rise.n.09'), Synset('raise.n.01'), Synset('hike.v.01'), Synset('hike.v.02')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# WORDNET CORPUS DEMO\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = 'hike' # taking hike as our word of interest\n",
    "\n",
    "# get word synsets\n",
    "word_synsets = wn.synsets(word)\n",
    "print word_synsets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset Name: hike.n.01\n",
      "POS Tag: n\n",
      "Definition: a long walk usually for exercise or pleasure\n",
      "Examples: [u'she enjoys a hike in her spare time']\n",
      "\n",
      "Synset Name: rise.n.09\n",
      "POS Tag: n\n",
      "Definition: an increase in cost\n",
      "Examples: [u'they asked for a 10% rise in rates']\n",
      "\n",
      "Synset Name: raise.n.01\n",
      "POS Tag: n\n",
      "Definition: the amount a salary is increased\n",
      "Examples: [u'he got a 3% raise', u'he got a wage hike']\n",
      "\n",
      "Synset Name: hike.v.01\n",
      "POS Tag: v\n",
      "Definition: increase\n",
      "Examples: [u'The landlord hiked up the rents']\n",
      "\n",
      "Synset Name: hike.v.02\n",
      "POS Tag: v\n",
      "Definition: walk a long way, as for pleasure or physical exercise\n",
      "Examples: [u'We were hiking in Colorado', u'hike the Rockies']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get details for each synonym in synset\n",
    "for synset in word_synsets:\n",
    "    print 'Synset Name:', synset.name()\n",
    "    print 'POS Tag:', synset.pos()\n",
    "    print 'Definition:', synset.definition()\n",
    "    print 'Examples:', synset.examples()\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 - nltk",
   "language": "python",
   "name": "python2-nltk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
